import torch
import os
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
from transformers import BertTokenizer
from collections import Counter

# ==========================================
# 1. è‡ªå®šä¹‰ Dataset ç±» (å…¼å®¹ PyTorch DataLoader)
# ==========================================
class NLPDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        # æ„é€ æ¯ä¸€ä¸ªæ ·æœ¬çš„å­—å…¸
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

# ==========================================
# 2. æ ¸å¿ƒæ•°æ®ç®¡ç†å™¨ç±»
# ==========================================
class DataManager:
    def __init__(self, args):
        """
        args: åŒ…å«å‘½ä»¤è¡Œå‚æ•°çš„å¯¹è±¡ (å¿…é¡»åŒ…å« .dataset, .model_type, .batch_size, .max_len)
        """
        self.dataset_name = args.dataset
        self.model_type = args.model_type # 'bert' æˆ– 'rnn'/'lstm'/'gru'/'transformer'
        self.batch_size = args.batch_size
        self.max_len = args.max_len
        
        # å­˜å‚¨æ„å»ºçš„è¯è¡¨ï¼ˆä»… RNN æ¨¡å¼ç”¨åˆ°ï¼‰
        self.vocab = None 
        self.word2idx = None

        self.data_path = os.path.join(os.getcwd(), 'data')

        # [æ–°å¢] å®šä¹‰é¢„è®­ç»ƒæ¨¡å‹ä¿å­˜è·¯å¾„
        self.model_cache_path = os.path.join(os.getcwd(), 'pre-model')
        
        # [æ–°å¢] å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        if not os.path.exists(self.model_cache_path):
            os.makedirs(self.model_cache_path)
            print(f"ğŸ“ Created model cache directory: {self.model_cache_path}")

    def load_data(self):
        """
        ä¸»å‡½æ•°ï¼šä¸‹è½½æ•°æ® -> é¢„å¤„ç† -> è¿”å› Loaders å’Œå…³é”®å‚æ•°
        Returns:
            train_loader, test_loader, output_dim, vocab_size
        """
        print(f"ğŸ”„ Loading dataset: {self.dataset_name}...")
        print(f"ğŸ“‚ Data will be cached at: {self.data_path}")
        
        # -------------------------------------------
        # A. åŠ è½½åŸå§‹æ•°æ® (Hugging Face Datasets)
        # -------------------------------------------
        if self.dataset_name == 'imdb':
            raw_dataset = load_dataset("imdb", cache_dir=self.data_path)
            text_col, label_col = 'text', 'label'
            output_dim = 2
        elif self.dataset_name == 'ag_news':
            raw_dataset = load_dataset("ag_news", cache_dir=self.data_path)
            text_col, label_col = 'text', 'label'
            output_dim = 4
        elif self.dataset_name == 'sst2':
            # SST-2 æ˜¯ GLUE benchmark çš„ä¸€éƒ¨åˆ†
            raw_dataset = load_dataset("glue", "sst2", cache_dir=self.data_path)
            text_col, label_col = 'sentence', 'label'
            output_dim = 2
        else:
            raise ValueError(f"Unknown dataset: {self.dataset_name}")

        # -------------------------------------------
        # B. æ ¹æ®æ¨¡å‹ç±»å‹è¿›è¡Œåˆ†è¯å¤„ç†
        # -------------------------------------------
        if self.model_type == 'bert':
            train_dataset, test_dataset = self._process_for_bert(raw_dataset, text_col, label_col)
            vocab_size = 0 # BERT è‡ªå¸¦ embeddingï¼Œä¸éœ€è¦æˆ‘ä»¬ä¼ å…¥ vocab_size
        else:
            train_dataset, test_dataset, vocab_size = self._process_for_rnn(raw_dataset, text_col, label_col)

        # -------------------------------------------
        # C. æ„é€  DataLoader
        # -------------------------------------------
        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)

        print(f"âœ… Data loaded successfully. Output Dim: {output_dim}, Vocab Size: {vocab_size}")
        return train_loader, test_loader, output_dim, vocab_size

    # ================= INTERNAL METHODS =================

    def _process_for_bert(self, dataset, text_col, label_col):
        """BERT ä¸“å±å¤„ç†ï¼šä½¿ç”¨ Hugging Face Tokenizer"""
        print("âš™ï¸ Processing for BERT (Tokenization & Padding)...")
        tokenizer = BertTokenizer.from_pretrained(
            'bert-base-uncased', 
            cache_dir=self.model_cache_path
        )

        def tokenize_function(examples):
            return tokenizer(examples[text_col], padding="max_length", truncation=True, max_length=self.max_len)

        # æ‰¹é‡å¤„ç†
        tokenized_train = dataset['train'].map(tokenize_function, batched=True)
        # éƒ¨åˆ†æ•°æ®é›†éªŒè¯é›†åå­—ä¸åŒï¼Œè¿™é‡Œåšä¸ªç®€å•çš„å…¼å®¹
        if self.dataset_name == 'sst2':
            val_split = 'validation'
        else:
            val_split = 'test' if 'test' in dataset else 'validation'
        tokenized_test = dataset[val_split].map(tokenize_function, batched=True)

        # è½¬æ¢ä¸º PyTorch æ ¼å¼
        train_ds = NLPDataset(
            encodings={'input_ids': tokenized_train['input_ids'], 'attention_mask': tokenized_train['attention_mask']},
            labels=tokenized_train[label_col]
        )
        test_ds = NLPDataset(
            encodings={'input_ids': tokenized_test['input_ids'], 'attention_mask': tokenized_test['attention_mask']},
            labels=tokenized_test[label_col]
        )
        return train_ds, test_ds

    def _process_for_rnn(self, dataset, text_col, label_col):
        """RNN/LSTM ä¸“å±å¤„ç†ï¼šæ„å»ºè¯è¡¨ + åºåˆ—åŒ–"""
        print("âš™ï¸ Building Vocabulary for RNN/LSTM...")
        
        # 1. æ„å»ºè¯è¡¨ (ä»…ä½¿ç”¨è®­ç»ƒé›†)
        tokens_list = [text.lower().split() for text in dataset['train'][text_col]]
        word_counts = Counter([token for line in tokens_list for token in line])
        
        # åªä¿ç•™å‡ºç°é¢‘ç‡æœ€é«˜çš„ Top N è¯ï¼Œé˜²æ­¢è¯è¡¨çˆ†ç‚¸
        MAX_VOCAB_SIZE = 25000 
        most_common = word_counts.most_common(MAX_VOCAB_SIZE)
        
        # ç‰¹æ®Š Token: <PAD> = 0, <UNK> = 1
        self.word2idx = {'<PAD>': 0, '<UNK>': 1}
        for idx, (word, _) in enumerate(most_common, start=2):
            self.word2idx[word] = idx
            
        vocab_size = len(self.word2idx)

        # 2. æ•°å€¼åŒ– & Padding å‡½æ•°
        def encode_and_pad(text_list):
            input_ids = []
            for text in text_list:
                tokens = text.lower().split()
                # è½¬æ¢: è¯ -> ID (ä¸å­˜åœ¨åˆ™ç”¨ UNK)
                ids = [self.word2idx.get(t, 1) for t in tokens]
                
                # æˆªæ–­æˆ–å¡«å……
                if len(ids) > self.max_len:
                    ids = ids[:self.max_len]
                else:
                    ids += [0] * (self.max_len - len(ids)) # Padding with 0
                input_ids.append(ids)
            return input_ids

        # 3. å¤„ç†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        train_ids = encode_and_pad(dataset['train'][text_col])
        if self.dataset_name == 'sst2':
            val_split = 'validation'
        else:
            val_split = 'test' if 'test' in dataset else 'validation'
        test_ids = encode_and_pad(dataset[val_split][text_col])

        train_ds = NLPDataset({'input_ids': train_ids}, dataset['train'][label_col])
        test_ds = NLPDataset({'input_ids': test_ids}, dataset[val_split][label_col])
        
        return train_ds, test_ds, vocab_size