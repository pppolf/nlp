import torch
import torch.nn as nn
import torch.optim as optim
import argparse
import time
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm
import os
import csv

# å¯¼å…¥æˆ‘ä»¬ä¹‹å‰å†™çš„æ¨¡å—
from utils.data_loader import DataManager
from models import make_model

# =============================================================================
# 0. è¾…åŠ©å‡½æ•°ï¼šè®¾ç½®éšæœºç§å­ (ä¿è¯å®éªŒå¯å¤ç°)
# =============================================================================
def set_seed(seed=42):
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    np.random.seed(seed)

# =============================================================================
# 1. è®­ç»ƒä¸è¯„ä¼°å‡½æ•°
# =============================================================================
def train(model, iterator, optimizer, criterion, device, model_type):
    model.train()
    epoch_loss = 0
    epoch_acc = 0
    
    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    pbar = tqdm(iterator, desc="Training", leave=False)
    
    for batch in pbar:
        # 1. è·å–æ•°æ®å¹¶ç§»åŠ¨åˆ° GPU
        # æ³¨æ„ï¼šdata_loader é‡Œçš„ batch æ˜¯ä¸€ä¸ªå­—å…¸
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)
        
        optimizer.zero_grad()
        
        # 2. å‰å‘ä¼ æ’­ (æ ¹æ®æ¨¡å‹ç±»å‹å¤„ç†è¾“å…¥)
        if model_type == 'bert':
            mask = batch['attention_mask'].to(device)
            predictions = model(input_ids, mask)
        else:
            # RNN/Transformer åªéœ€è¦ input_ids
            predictions = model(input_ids)
            
        # 3. è®¡ç®— Loss å’Œ Accuracy
        loss = criterion(predictions, labels)
        
        # è®¡ç®—å‡†ç¡®ç‡ (å–æœ€å¤§æ¦‚ç‡çš„ç´¢å¼•)
        preds = predictions.argmax(dim=1)
        acc = (preds == labels).float().mean()
        
        # 4. åå‘ä¼ æ’­ä¸ä¼˜åŒ–
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
        epoch_acc += acc.item()
        
        # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºçš„å½“å‰ loss
        pbar.set_postfix({'loss': f'{loss.item():.4f}'})
        
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

def evaluate(model, iterator, criterion, device, model_type):
    model.eval()
    epoch_loss = 0
    epoch_acc = 0
    
    with torch.no_grad(): # è¯„ä¼°æ¨¡å¼ä¸è®¡ç®—æ¢¯åº¦ï¼ŒèŠ‚çœæ˜¾å­˜
        for batch in iterator:
            input_ids = batch['input_ids'].to(device)
            labels = batch['labels'].to(device)
            
            if model_type == 'bert':
                mask = batch['attention_mask'].to(device)
                predictions = model(input_ids, mask)
            else:
                predictions = model(input_ids)
                
            loss = criterion(predictions, labels)
            preds = predictions.argmax(dim=1)
            acc = (preds == labels).float().mean()
            
            epoch_loss += loss.item()
            epoch_acc += acc.item()
            
    return epoch_loss / len(iterator), epoch_acc / len(iterator)

# =============================================================================
# 2. ç»˜å›¾å‡½æ•° (ç›´æ¥ç”Ÿæˆè®ºæ–‡å¯ç”¨çš„å›¾)
# =============================================================================
def plot_metrics(history, save_path):
    epochs = range(1, len(history['train_loss']) + 1)
    
    # åˆ›å»ºç”»å¸ƒ
    plt.figure(figsize=(12, 5))
    
    # å­å›¾1: Loss
    plt.subplot(1, 2, 1)
    plt.plot(epochs, history['train_loss'], 'b-', label='Train Loss')
    plt.plot(epochs, history['val_loss'], 'r-', label='Val Loss')
    plt.title('Training & Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # å­å›¾2: Accuracy
    plt.subplot(1, 2, 2)
    plt.plot(epochs, history['train_acc'], 'b-', label='Train Acc')
    plt.plot(epochs, history['val_acc'], 'r-', label='Val Acc')
    plt.title('Training & Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    
    # ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plt.savefig(os.path.join(save_path, 'result_plot.png'))
    print(f"ğŸ“Š Plot saved to {save_path}/result_plot.png")

# =============================================================================
# 3. ä¸»å‡½æ•°
# =============================================================================
def main():
    # A. å‘½ä»¤è¡Œå‚æ•°å®šä¹‰
    parser = argparse.ArgumentParser(description='NLP Model Experiment')
    
    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--dataset', type=str, default='imdb', choices=['imdb', 'ag_news', 'sst2'], help='Dataset name')
    parser.add_argument('--max_len', type=int, default=256, help='Max sequence length')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model_type', type=str, default='lstm', choices=['rnn', 'lstm', 'gru', 'transformer', 'bert'], help='Model architecture')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=10)
    parser.add_argument('--batch_size', type=int, default=64)
    parser.add_argument('--lr', type=float, default=1e-3, help='Learning rate (use 2e-5 for BERT)')
    parser.add_argument('--seed', type=int, default=42)
    
    args = parser.parse_args()
    
    # B. åˆå§‹åŒ–è®¾ç½®
    set_seed(args.seed)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åˆ›å»ºä¿å­˜ç»“æœçš„æ–‡ä»¶å¤¹
    result_dir = f"results/{args.dataset}_{args.model_type}"
    if not os.path.exists(result_dir):
        os.makedirs(result_dir)
    
    print(f"{'='*30}")
    print(f"ğŸš€ Experiment: {args.model_type.upper()} on {args.dataset.upper()}")
    print(f"ğŸ’» Device: {device}")
    print(f"ğŸ“‚ Results will be saved to: {result_dir}")
    print(f"{'='*30}")

    # C. åŠ è½½æ•°æ®
    data_manager = DataManager(args)
    train_loader, test_loader, output_dim, vocab_size = data_manager.load_data()

    # D. æ„å»ºæ¨¡å‹
    model = make_model(args, vocab_size, output_dim, device)
    
    # è®¡ç®—å‚æ•°é‡
    count_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"ğŸ§  The model has {count_parameters:,} trainable parameters")

    # E. å®šä¹‰ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    # BERT é€šå¸¸éœ€è¦æ›´å°çš„å­¦ä¹ ç‡ (å¦‚ 2e-5)ï¼ŒRNN å¯ä»¥ç”¨ 1e-3
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    criterion = nn.CrossEntropyLoss()
    
    # å°†æ¨¡å‹å’ŒæŸå¤±å‡½æ•°ç§»åŠ¨åˆ°è®¾å¤‡
    model = model.to(device)
    criterion = criterion.to(device)

    # F. è®­ç»ƒå¾ªç¯
    # å®šä¹‰æ—¥å¿—æ–‡ä»¶è·¯å¾„
    log_file = 'epoch_logs.csv'
    
    # å¦‚æœæ˜¯ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œå†™å…¥è¡¨å¤´ (Dataset, Model, Epoch, Accuracy)
    if not os.path.exists(log_file):
        with open(log_file, mode='w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['Dataset', 'Model', 'Epoch', 'Accuracy'])

    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}
    best_valid_loss = float('inf')
    
    start_time = time.time()
    
    for epoch in range(args.epochs):
        epoch_start = time.time()
        
        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device, args.model_type)
        valid_loss, valid_acc = evaluate(model, test_loader, criterion, device, args.model_type)
        
        with open(log_file, mode='a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            # å†™å…¥ï¼šæ•°æ®é›†å, æ¨¡å‹å, å½“å‰Epoch(ä»1å¼€å§‹), å½“å‰å‡†ç¡®ç‡
            writer.writerow([args.dataset, args.model_type, epoch + 1, valid_acc])

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(valid_loss)
        history['val_acc'].append(valid_acc)
        
        epoch_end = time.time()
        epoch_mins, epoch_secs = divmod(epoch_end - epoch_start, 60)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), os.path.join(result_dir, 'best_model.pt'))
            saved_msg = "ğŸ”¥ (Saved)"
        else:
            saved_msg = ""
            
        print(f'Epoch: {epoch+1:02} | Time: {int(epoch_mins)}m {int(epoch_secs)}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')
        print(f'\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% {saved_msg}')
    
    total_time = time.time() - start_time
    print(f"ğŸ Training finished in {int(total_time/60)}m {int(total_time%60)}s")

    # G. ç»˜å›¾ä¸ä¿å­˜
    plot_metrics(history, result_dir)

if __name__ == '__main__':
    main()