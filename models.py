import os
import torch
import torch.nn as nn
from transformers import BertModel

# =============================================================================
# 1. ç»Ÿä¸€çš„ RNN/LSTM/GRU æ¨¡å‹
# =============================================================================
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim, 
                 n_layers=2, bidirectional=True, dropout=0.5, model_type='lstm'):
        super().__init__()
        self.model_type = model_type.lower()
        
        # è¯åµŒå…¥å±‚
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # å®šä¹‰å¾ªç¯å±‚
        # batch_first=True è®©è¾“å…¥å˜ä¸º [batch, seq_len, dim]
        rnn_args = {
            'input_size': embed_dim,
            'hidden_size': hidden_dim,
            'num_layers': n_layers,
            'bidirectional': bidirectional,
            'batch_first': True,
            'dropout': dropout if n_layers > 1 else 0
        }
        
        if self.model_type == 'lstm':
            self.rnn = nn.LSTM(**rnn_args)
        elif self.model_type == 'gru':
            self.rnn = nn.GRU(**rnn_args)
        else: # Basic RNN
            self.rnn = nn.RNN(**rnn_args)
            
        # å…¨è¿æ¥å±‚ (åˆ†ç±»å™¨)
        # å¦‚æœæ˜¯åŒå‘ï¼Œhidden_dim éœ€è¦ x2
        self.fc_input_dim = hidden_dim * 2 if bidirectional else hidden_dim
        self.fc = nn.Linear(self.fc_input_dim, output_dim)
        
        # Dropout é˜²æ­¢è¿‡æ‹Ÿåˆ
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        # text: [batch_size, seq_len]
        
        # 1. Embedding
        embedded = self.dropout(self.embedding(text)) # [batch, seq_len, embed]
        
        # 2. RNN Layer
        # output: æ¯ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        # hidden: æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ (ç”¨äºåˆ†ç±»)
        if self.model_type == 'lstm':
            output, (hidden, cell) = self.rnn(embedded)
        else:
            output, hidden = self.rnn(embedded)
            
        # 3. è·å–æœ€åä¸€ä¸ªéšè—çŠ¶æ€
        # hidden shape: [num_layers * num_directions, batch, hidden_dim]
        # æˆ‘ä»¬éœ€è¦æŠŠåŒå‘çš„æœ€åä¸¤ä¸ª hidden æ‹¼æ¥èµ·æ¥
        if self.rnn.bidirectional:
            # å–æœ€åä¸¤å±‚ (æ­£å‘æœ€åä¸€å±‚ + åå‘æœ€åä¸€å±‚)
            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)
        else:
            hidden = hidden[-1,:,:]
            
        # 4. Classification
        return self.fc(self.dropout(hidden))

# =============================================================================
# 2. Transformer (Encoder Only) æ¨¡å‹
# =============================================================================
class TransformerClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, n_heads, hidden_dim, n_layers, output_dim, max_len=256, dropout=0.5):
        super().__init__()
        
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        
        # ç®€å•çš„ä½ç½®ç¼–ç  (Learnable Positional Encoding)
        # ä¹Ÿå¯ä»¥ç”¨ sin/cos å›ºå®šç¼–ç ï¼Œä½† Learnable åœ¨å¤§ä½œä¸šé‡Œæ›´ç®€å•ä¸”æ•ˆæœå¤Ÿç”¨
        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))
        
        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim, 
            nhead=n_heads, 
            dim_feedforward=hidden_dim, 
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)
        
        self.fc = nn.Linear(embed_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, text):
        # text: [batch, seq_len]
        batch_size, seq_len = text.shape
        
        # 1. Embedding + Positional Encoding
        # æˆªå–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç 
        pos_embed = self.pos_embedding[:, :seq_len, :]
        embedded = self.dropout(self.embedding(text) + pos_embed)
        
        # 2. Transformer Forward
        # mask ç”¨äºå¿½ç•¥ padding (0) çš„ä½ç½®ï¼Œé¿å… attention å…³æ³¨åˆ° padding
        # src_key_padding_mask: [batch, seq_len] (True for padding)
        padding_mask = (text == 0)
        
        transformer_out = self.transformer_encoder(embedded, src_key_padding_mask=padding_mask)
        # shape: [batch, seq_len, embed_dim]
        
        # 3. Pooling (èšåˆç­–ç•¥)
        # è¿™é‡Œä½¿ç”¨ Mean Pooling (å–æ‰€æœ‰é padding è¯å‘é‡çš„å¹³å‡å€¼)
        # ä¸ºäº†ç®€å•ï¼Œç›´æ¥å¯¹æ‰€æœ‰è¾“å‡ºå–å¹³å‡ (ç¨å¾®ç²—ç³™ä½†æœ‰æ•ˆ)
        output = transformer_out.mean(dim=1) 
        
        return self.fc(output)

# =============================================================================
# 3. BERT æ¨¡å‹
# =============================================================================
class BERTClassifier(nn.Module):
    def __init__(self, output_dim, cache_dir=None, freeze_bert=False):
        super().__init__()
        # è‡ªåŠ¨ä¸‹è½½/åŠ è½½é¢„è®­ç»ƒæƒé‡
        self.bert = BertModel.from_pretrained(
            'bert-base-uncased',
            cache_dir=cache_dir
        )
        
        # æ˜¯å¦å†»ç»“ BERT å‚æ•° (åªè®­ç»ƒæœ€åçš„åˆ†ç±»å±‚)
        if freeze_bert:
            for param in self.bert.parameters():
                param.requires_grad = False
                
        # è¿™é‡Œçš„ hidden_size é€šå¸¸æ˜¯ 768
        self.fc = nn.Linear(self.bert.config.hidden_size, output_dim)

    def forward(self, input_ids, attention_mask):
        # BERT forward
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        
        # pooler_output æ˜¯ [CLS] token ç»è¿‡ä¸€å±‚çº¿æ€§å±‚åçš„è¾“å‡º
        # ä¸“é—¨è®¾è®¡ç”¨äºåˆ†ç±»ä»»åŠ¡
        return self.fc(outputs.pooler_output)

# =============================================================================
# 4. å·¥å‚å‡½æ•°ï¼šä¸€é”®ç”Ÿæˆæ¨¡å‹
# =============================================================================
def make_model(args, vocab_size, output_dim, device):
    """
    æ ¹æ®å‘½ä»¤è¡Œå‚æ•° args è‡ªåŠ¨é€‰æ‹©å¹¶åˆå§‹åŒ–æ¨¡å‹
    """
    model_type = args.model_type.lower()
    pre_model_path = os.path.join(os.getcwd(), 'pre-model')
    
    print(f"ğŸ—ï¸ Building Model: {model_type.upper()}...")
    
    if model_type == 'bert':
        model = BERTClassifier(output_dim, cache_dir=pre_model_path)
        
    elif model_type in ['rnn', 'lstm', 'gru']:
        # å¯ä»¥åœ¨ args é‡ŒåŠ è¿™äº›å‚æ•°ï¼Œè¿™é‡Œå…ˆç»™é»˜è®¤å€¼
        model = RNNClassifier(
            vocab_size=vocab_size,
            embed_dim=100,      # è¯å‘é‡ç»´åº¦
            hidden_dim=256,     # éšè—å±‚ç»´åº¦
            output_dim=output_dim,
            n_layers=2,         # å±‚æ•°
            bidirectional=True, # åŒå‘
            dropout=0.5,
            model_type=model_type
        )
        
    elif model_type == 'transformer':
        # æ³¨æ„: embed_dim å¿…é¡»èƒ½è¢« n_heads æ•´é™¤
        # è¿™é‡Œå¼ºåˆ¶è®¾ embed_dim=128, n_heads=4
        model = TransformerClassifier(
            vocab_size=vocab_size,
            embed_dim=128,      
            n_heads=4,
            hidden_dim=256,
            n_layers=2,
            output_dim=output_dim,
            max_len=args.max_len,
            dropout=0.5
        )
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    return model.to(device)